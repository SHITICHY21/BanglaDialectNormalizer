{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":119724,"databundleVersionId":14372637,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[{"file_id":"1ldHqC_Q7YOouw5BwSMXMZcORjzdhklZm","timestamp":1762977052319}]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets librosa soundfile jiwer accelerate evaluate peft\n!pip install -q bitsandbytes  # For 8-bit training if needed\n!pip install -q av ffmpeg-python  # For audio decoding support\n!apt-get -qq install -y ffmpeg  # System-level ffmpeg","metadata":{"trusted":true,"id":"JyiAVovNnTi_","execution":{"iopub.status.busy":"2025-11-16T10:18:37.675494Z","iopub.execute_input":"2025-11-16T10:18:37.675991Z","iopub.status.idle":"2025-11-16T10:20:10.323238Z","shell.execute_reply.started":"2025-11-16T10:18:37.675967Z","shell.execute_reply":"2025-11-16T10:20:10.322238Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport librosa\nimport soundfile as sf\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport re\n\nfrom transformers import (\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n)\nfrom datasets import Dataset, Audio\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport evaluate\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\nprint(f\"CUDA Version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\")\n\n# Set paths\nTRAIN_PATH = \"/kaggle/input/shobdotori/Train\"\nTRAIN_ANNOTATION_PATH = \"/kaggle/input/shobdotori/Train_annotation\"\nTEST_PATH = \"/kaggle/input/shobdotori/Test\"\nOUTPUT_DIR = \"./whisper-bengali-lora\"\nSUBMISSION_FILE = \"submission.csv\"","metadata":{"trusted":true,"id":"-oRSgyhJnTjC","execution":{"iopub.status.busy":"2025-11-16T10:20:10.325025Z","iopub.execute_input":"2025-11-16T10:20:10.325306Z","iopub.status.idle":"2025-11-16T10:20:37.211977Z","shell.execute_reply.started":"2025-11-16T10:20:10.325282Z","shell.execute_reply":"2025-11-16T10:20:37.211097Z"}},"outputs":[{"name":"stderr","text":"2025-11-16 10:20:19.333213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763288419.497909      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763288419.544218      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Using device: cuda\nGPU: Tesla P100-PCIE-16GB\nCUDA Version: 12.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def load_training_data():\n    \"\"\"Load all training data from regional folders and their annotations\"\"\"\n    all_data = []\n\n    # Get all region folders\n    region_folders = [f for f in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, f))]\n\n    print(f\"Found {len(region_folders)} regions\")\n\n    for region in tqdm(region_folders, desc=\"Loading regions\"):\n        # Load annotation CSV for this region\n        annotation_file = os.path.join(TRAIN_ANNOTATION_PATH, f\"{region}.csv\")\n\n        if not os.path.exists(annotation_file):\n            print(f\"Warning: No annotation file for {region}\")\n            continue\n\n        # Read annotations\n        annotations_df = pd.read_csv(annotation_file)\n\n        # Strip whitespace from column names\n        annotations_df.columns = annotations_df.columns.str.strip()\n\n        # Get audio folder path\n        audio_folder = os.path.join(TRAIN_PATH, region)\n\n        # Process each annotation\n        for idx, row in annotations_df.iterrows():\n            audio_file = row['audio'].strip() if 'audio' in annotations_df.columns else row.iloc[0]\n            text = row['text'].strip() if 'text' in annotations_df.columns else row.iloc[1]\n\n            audio_path = os.path.join(audio_folder, audio_file)\n\n            if os.path.exists(audio_path):\n                all_data.append({\n                    'audio': audio_path,\n                    'text': text,\n                    'region': region\n                })\n\n    print(f\"\\nTotal training samples: {len(all_data)}\")\n    return pd.DataFrame(all_data)\n\n# Load training data\ntrain_df = load_training_data()\nprint(f\"\\nSample data:\")\nprint(train_df.head())","metadata":{"trusted":true,"id":"vBxDPKNrnTjD","execution":{"iopub.status.busy":"2025-11-16T10:20:37.213100Z","iopub.execute_input":"2025-11-16T10:20:37.213701Z","iopub.status.idle":"2025-11-16T10:20:45.451815Z","shell.execute_reply.started":"2025-11-16T10:20:37.213680Z","shell.execute_reply":"2025-11-16T10:20:45.451043Z"}},"outputs":[{"name":"stdout","text":"Found 20 regions\n","output_type":"stream"},{"name":"stderr","text":"Loading regions: 100%|██████████| 20/20 [00:08<00:00,  2.43it/s]","output_type":"stream"},{"name":"stdout","text":"\nTotal training samples: 3350\n\nSample data:\n                                               audio  \\\n0  /kaggle/input/shobdotori/Train/Mymensingh/fema...   \n1  /kaggle/input/shobdotori/Train/Mymensingh/fema...   \n2  /kaggle/input/shobdotori/Train/Mymensingh/fema...   \n3  /kaggle/input/shobdotori/Train/Mymensingh/fema...   \n4  /kaggle/input/shobdotori/Train/Mymensingh/male...   \n\n                              text      region  \n0  আজ সকালে আমি বাজারে গিয়েছিলাম।  Mymensingh  \n1       তুমি কি নতুন বই পড়তে চাও?  Mymensingh  \n2            আকাশে আজ মেঘ জমে আছে।  Mymensingh  \n3   আমি বন্ধুদের সাথে খেলা খেলেছি।  Mymensingh  \n4   দরজাটা ধীরে ধীরে বন্ধ করে দাও।  Mymensingh  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q transformers[sentencepiece] datasets torchaudio librosa sacrebleu evaluate tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T11:07:03.594160Z","iopub.execute_input":"2025-11-16T11:07:03.594916Z","iopub.status.idle":"2025-11-16T11:07:07.299696Z","shell.execute_reply.started":"2025-11-16T11:07:03.594890Z","shell.execute_reply":"2025-11-16T11:07:07.298662Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torchaudio\nimport librosa\nimport re\n\nfrom transformers import (\n    AutoProcessor,\n    AutoModelForSpeechSeq2Seq,\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n)\n\n# ---------------------------\n# Reproducibility & device\n# ---------------------------\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ---------------------------\n# Paths / config (শব্দতরী)\n# ---------------------------\nDATA_DIR = \"/kaggle/input/shobdotori\"   # <- tumi ja bolechho\n\n# NOTE: folder nameগুলো screenshot অনুযায়ী capital:\nTRAIN_AUDIO_DIR = os.path.join(DATA_DIR, \"Train\")\nTEST_AUDIO_DIR  = os.path.join(DATA_DIR, \"Test\")\nTRAIN_ANN_DIR   = os.path.join(DATA_DIR, \"Train_annotation\")\n\nSAMPLE_SUB_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n\nprint(\"\\nChecking paths...\")\nprint(\"TRAIN_AUDIO_DIR :\", TRAIN_AUDIO_DIR, os.path.exists(TRAIN_AUDIO_DIR))\nprint(\"TEST_AUDIO_DIR  :\", TEST_AUDIO_DIR, os.path.exists(TEST_AUDIO_DIR))\nprint(\"TRAIN_ANN_DIR   :\", TRAIN_ANN_DIR, os.path.exists(TRAIN_ANN_DIR))\nprint(\"SAMPLE_SUB_PATH :\", SAMPLE_SUB_PATH, os.path.exists(SAMPLE_SUB_PATH))\n\nif not os.path.exists(TRAIN_ANN_DIR):\n    raise FileNotFoundError(f\"Train_annotation dir not found at: {TRAIN_ANN_DIR}\")\n\nif not os.path.exists(SAMPLE_SUB_PATH):\n    raise FileNotFoundError(f\"sample_submission.csv not found at: {SAMPLE_SUB_PATH}\")\n\n# ---------------------------\n# Load all Train_annotation/*.csv and concat\n# ---------------------------\ntrain_dfs = []\nfor fname in sorted(os.listdir(TRAIN_ANN_DIR)):\n    if not fname.lower().endswith(\".csv\"):\n        continue\n    csv_path = os.path.join(TRAIN_ANN_DIR, fname)\n    df_reg = pd.read_csv(csv_path)\n\n    # region name from file, e.g. \"Barisal.csv\" -> \"Barisal\"\n    region = os.path.splitext(fname)[0]\n    df_reg[\"region\"] = region\n\n    train_dfs.append(df_reg)\n\ntrain_df = pd.concat(train_dfs, ignore_index=True)\n\nprint(\"\\n✅ Loaded Train_annotation CSVs:\", len(train_dfs), \"files\")\nprint(\"Columns:\", train_df.columns.tolist())\nprint(\"Total rows in train_df:\", len(train_df))\nprint(train_df.head())\n\n# sample_submission for test list\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\nprint(\"\\n✅ Loaded sample_submission.csv with\", len(sample_sub), \"rows\")\nprint(sample_sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T11:29:03.295828Z","iopub.execute_input":"2025-11-16T11:29:03.296459Z","iopub.status.idle":"2025-11-16T11:29:03.397278Z","shell.execute_reply.started":"2025-11-16T11:29:03.296433Z","shell.execute_reply":"2025-11-16T11:29:03.396299Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nChecking paths...\nTRAIN_AUDIO_DIR : /kaggle/input/shobdotori/Train True\nTEST_AUDIO_DIR  : /kaggle/input/shobdotori/Test True\nTRAIN_ANN_DIR   : /kaggle/input/shobdotori/Train_annotation True\nSAMPLE_SUB_PATH : /kaggle/input/shobdotori/sample_submission.csv True\n\n✅ Loaded Train_annotation CSVs: 20 files\nColumns: ['audio', 'text', 'region']\nTotal rows in train_df: 3350\n                  audio                             text   region\n0  female_barisal_1.wav  আজ সকালে আমি বাজারে গিয়েছিলাম।  Barisal\n1  female_barisal_3.wav            আকাশে আজ মেঘ জমে আছে।  Barisal\n2  female_barisal_4.wav   আমি বন্ধুদের সাথে খেলা খেলেছি।  Barisal\n3  female_barisal_5.wav   দরজাটা ধীরে ধীরে বন্ধ করে দাও।  Barisal\n4  female_barisal_6.wav    তুমি কি আমাকে পানি দিতে পারো?  Barisal\n\n✅ Loaded sample_submission.csv with 450 rows\n          audio          text\n0  test_001.wav  আমি ভাত খাই।\n1  test_002.wav  আমি ভাত খাই।\n2  test_003.wav  আমি ভাত খাই।\n3  test_004.wav  আমি ভাত খাই।\n4  test_005.wav  আমি ভাত খাই।\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = \"openai/whisper-medium\"\n\nprint(f\"Loading model: {MODEL_NAME}\")\nprint(\"This may take a few minutes...\")\n\n# Load processor\nprocessor = WhisperProcessor.from_pretrained(\n    MODEL_NAME,\n    language=\"bengali\",\n    task=\"transcribe\"\n)\n\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float32,\n    low_cpu_mem_usage=True\n)\n\n\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n    language=\"bn\",   # short code ব্যবহার করো\n    task=\"transcribe\"\n)\nmodel.config.suppress_tokens = []\nmodel.config.use_cache = True\n\n# Prepare model for training\nmodel.config.forced_decoder_ids = None\nmodel.config.suppress_tokens = []\nmodel.config.use_cache = False\n\n# Enable gradient checkpointing for memory efficiency\nmodel.gradient_checkpointing_enable()\n\n# Apply LoRA - Target only attention layers\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],  # একটু বেশি capacity\n    lora_dropout=0.05,\n    bias=\"none\",\n)\n\n# Apply LoRA to model.model (inner Whisper model)\nmodel.model = get_peft_model(model.model, lora_config)\nmodel = model.to(device)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\n✓ LoRA applied successfully\")\nprint(f\"  Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\nprint(f\"  Total params: {total_params:,}\")","metadata":{"trusted":true,"id":"2QLQV7SHnTjE","execution":{"iopub.status.busy":"2025-11-16T10:20:50.396384Z","iopub.status.idle":"2025-11-16T10:20:50.396601Z","shell.execute_reply.started":"2025-11-16T10:20:50.396498Z","shell.execute_reply":"2025-11-16T10:20:50.396507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_dataset(df, processor, test_size=0.1):\n    \"\"\"Prepare dataset for training\"\"\"\n\n    def prepare_dataset_batch(batch):\n        \"\"\"Process a single batch\"\"\"\n        try:\n            audio_path = batch[\"audio\"]\n            # audio_array, sampling_rate = librosa.load(audio_path, sr=16000, mono=True)\n            audio_array, sampling_rate = librosa.load(audio_path, sr=16000, mono=True)\n            audio_array, _ = librosa.effects.trim(audio_array, top_db=20)\n            audio_array = audio_array / (np.max(np.abs(audio_array)) + 1e-8)\n\n\n            # Compute input features\n            input_features = processor.feature_extractor(\n                audio_array,\n                sampling_rate=16000\n            ).input_features[0]\n\n            # Encode target text\n            labels = processor.tokenizer(batch[\"text\"]).input_ids\n\n            return {\n                \"input_features\": input_features,\n                \"labels\": labels\n            }\n        except Exception as e:\n            print(f\"Error processing {batch['audio']}: {e}\")\n            return {\n                \"input_features\": np.zeros((80, 3000)),\n                \"labels\": [processor.tokenizer.pad_token_id]\n            }\n\n    dataset = Dataset.from_pandas(df[['audio', 'text']])\n    dataset = dataset.train_test_split(test_size=test_size, seed=42)\n\n    print(\"Processing training dataset...\")\n    train_dataset = dataset[\"train\"].map(\n        prepare_dataset_batch,\n        remove_columns=dataset[\"train\"].column_names,\n        desc=\"Processing train\",\n    )\n\n    print(\"Processing validation dataset...\")\n    val_dataset = dataset[\"test\"].map(\n        prepare_dataset_batch,\n        remove_columns=dataset[\"test\"].column_names,\n        desc=\"Processing validation\",\n    )\n\n    return {\"train\": train_dataset, \"test\": val_dataset}\n\nprint(\"Preparing dataset...\")\ndataset = prepare_dataset(train_df, processor)\nprint(f\"\\nTrain samples: {len(dataset['train'])}\")\nprint(f\"Validation samples: {len(dataset['test'])}\")  ","metadata":{"trusted":true,"id":"fD4nEMTsnTjE","execution":{"iopub.status.busy":"2025-11-16T10:20:50.397819Z","iopub.status.idle":"2025-11-16T10:20:50.398140Z","shell.execute_reply.started":"2025-11-16T10:20:50.397978Z","shell.execute_reply":"2025-11-16T10:20:50.397993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        labels = labels_batch[\"input_ids\"].masked_fill(\n            labels_batch.attention_mask.ne(1), -100\n        )\n\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n        return batch\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\nprint(\"✓ Data collator initialized\")","metadata":{"trusted":true,"id":"udZTA61SnTjF","execution":{"iopub.status.busy":"2025-11-16T10:20:50.399210Z","iopub.status.idle":"2025-11-16T10:20:50.399534Z","shell.execute_reply.started":"2025-11-16T10:20:50.399357Z","shell.execute_reply":"2025-11-16T10:20:50.399371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    \"\"\"Compute WER metric\"\"\"\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n\n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n    return {\"wer\": wer}\n\nprint(\"✓ Metrics function initialized\")","metadata":{"trusted":true,"id":"JPMdbIzInTjF","execution":{"iopub.status.busy":"2025-11-16T10:20:50.401058Z","iopub.status.idle":"2025-11-16T10:20:50.401365Z","shell.execute_reply.started":"2025-11-16T10:20:50.401214Z","shell.execute_reply":"2025-11-16T10:20:50.401228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2, # REDUCED from 8\n    gradient_accumulation_steps=8,  # INCREASED from 2\n    learning_rate=1e-4,  # Higher LR for faster convergence\n    warmup_ratio=0.05,  # REDUCED\n    num_train_epochs=2,  # REDUCED from 5 for faster training\n    gradient_checkpointing=True,  # ENABLED\n    fp16=True,  # DISABLED - use FP32 for training stability\n    eval_strategy=\"steps\",\n    # per_device_eval_batch_size=4,  # REDUCED\n    predict_with_generate=True,\n    generation_max_length=225,\n    save_steps=400,\n    eval_steps=400,\n    logging_steps=50,\n    logging_first_step=True,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    push_to_hub=False,\n    save_total_limit=2,  # REDUCED to save space\n    dataloader_num_workers=2,\n    remove_unused_columns=False,\n    label_names=[\"labels\"],\n    optim=\"adamw_torch\",  # Explicit optimizer\n)\n\nprint(\"✓ Training arguments configured\")","metadata":{"trusted":true,"id":"-kSGOobhnTjF","execution":{"iopub.status.busy":"2025-11-16T10:20:50.402901Z","iopub.status.idle":"2025-11-16T10:20:50.403207Z","shell.execute_reply.started":"2025-11-16T10:20:50.403054Z","shell.execute_reply":"2025-11-16T10:20:50.403068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    processing_class=processor,\n)\n\nprint(\"✓ Trainer initialized successfully\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Total training steps: ~{len(dataset['train']) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")","metadata":{"trusted":true,"id":"3co9HtqsnTjG","execution":{"iopub.status.busy":"2025-11-16T10:20:50.404249Z","iopub.status.idle":"2025-11-16T10:20:50.404564Z","shell.execute_reply.started":"2025-11-16T10:20:50.404397Z","shell.execute_reply":"2025-11-16T10:20:50.404423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport gc\nimport torch\nfrom transformers import logging as hf_logging\n\nhf_logging.set_verbosity_info()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\n\ntry:\n    sys.stdout.flush()\n\n    # Clear cache before training (only if CUDA is available)\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\n    # Train the model\n    train_result = trainer.train()\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"✓ TRAINING COMPLETED!\")\n    print(\"=\"*60)\n    # Some Trainer versions use train_result.training_loss; fallback to metrics if needed\n    tr_loss = getattr(train_result, \"training_loss\", None)\n    if tr_loss is None and hasattr(train_result, \"metrics\"):\n        tr_loss = train_result.metrics.get(\"train_loss\", None)\n    if tr_loss is not None:\n        print(f\"Training loss: {tr_loss:.4f}\")\n\n    # Save the model\n    print(\"\\nSaving model...\")\n    trainer.save_model(OUTPUT_DIR)\n    processor.save_pretrained(OUTPUT_DIR)\n    print(f\"✓ Model and processor saved to {OUTPUT_DIR}\")\n\nexcept Exception as e:\n    print(\"\\n\" + \"=\"*60)\n    print(\" TRAINING ERROR\")\n    print(\"=\"*60)\n    print(f\"Error: {str(e)}\")\n    import traceback\n    traceback.print_exc()\n    raise ","metadata":{"trusted":true,"id":"hx1fsRNlnTjG","execution":{"iopub.status.busy":"2025-11-16T10:20:50.405666Z","iopub.status.idle":"2025-11-16T10:20:50.405991Z","shell.execute_reply.started":"2025-11-16T10:20:50.405835Z","shell.execute_reply":"2025-11-16T10:20:50.405849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_bengali_text(text):\n    \"\"\"Clean and normalize Bengali text\"\"\"\n    text = re.sub(r'\\s+', ' ', text)   \n    text = text.strip() \n    text = text.replace('।।', '।')\n    return text  \n\ndef transcribe_audio_enhanced(audio_path, model, processor, device):\n    \"\"\"FIXED: Enhanced transcription with proper dtype handling\"\"\"\n    try:\n        # Load and preprocess audio\n        audio_input, sr = librosa.load(audio_path, sr=16000, mono=True)\n        audio_input, _ = librosa.effects.trim(audio_input, top_db=20)\n        audio_input = audio_input / (np.max(np.abs(audio_input)) + 1e-8)\n\n        # Process - returns dict with input_features\n        inputs = processor(\n            audio_input,\n            sampling_rate=16000,\n            return_tensors=\"pt\"\n        )\n\n\n        input_features = inputs.input_features.to(device)\n        if model.dtype == torch.float16:\n            input_features = input_features.half()\n        elif model.dtype == torch.float32:\n            input_features = input_features.float()\n\n        # Set forced decoder IDs for Bengali\n        forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"bengali\", task=\"transcribe\")\n\n        # Generate with proper settings\n        with torch.no_grad():\n            predicted_ids = model.generate(\n                input_features,\n                forced_decoder_ids=forced_decoder_ids,\n                max_length=225,\n                num_beams=4,\n                early_stopping=True,\n                no_repeat_ngram_size=3,\n                temperature=0.0,  # Deterministic\n            )\n\n        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n        return clean_bengali_text(transcription) if transcription else \"আমি ভাত খাই।\"\n\n    except Exception as e:\n        print(f\"Error processing {audio_path}: {e}\")\n        return \"আমি ভাত খাই।\"\n\nprint(\"✓ Transcription function ready\")","metadata":{"trusted":true,"id":"MstcZRa6nTjH","execution":{"iopub.status.busy":"2025-11-16T10:20:50.406928Z","iopub.status.idle":"2025-11-16T10:20:50.407232Z","shell.execute_reply.started":"2025-11-16T10:20:50.407079Z","shell.execute_reply":"2025-11-16T10:20:50.407093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"PREPARING FOR INFERENCE\")\nprint(\"=\"*60)\n\nmodel = model.to(torch.float16)\nprint(\"✓ Model converted to FP16 for inference\")\n\n# Clear cache\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"GENERATING PREDICTIONS FOR TEST SET\")\nprint(\"=\"*60)\n\ntest_files = sorted([f for f in os.listdir(TEST_PATH) if f.endswith('.wav')])\nprint(f\"\\nFound {len(test_files)} test files to process\")\n\nif len(test_files) == 0:\n    print(\"No test files found!\")\nelse:\n    results = []\n    model.eval()\n\n    print(\"\\nProcessing test files...\")\n    for test_file in tqdm(test_files, desc=\"Transcribing audio\"):\n        audio_path = os.path.join(TEST_PATH, test_file)\n        transcription = transcribe_audio_enhanced(audio_path, model, processor, device)\n        results.append({'audio': test_file, 'text': transcription})\n\n    # Create submission dataframe\n    submission_df = pd.DataFrame(results)\n    submission_df.to_csv(SUBMISSION_FILE, index=False, encoding='utf-8')\n\n    print(f\"\\n{'='*60}\")\n    print(f\"✓ SUBMISSION FILE CREATED: {SUBMISSION_FILE}\")\n    print(f\"{'='*60}\")\n    print(f\"Total predictions: {len(submission_df)}\")\n    print(f\"\\nSample predictions (first 10):\")\n    print(submission_df.head(10).to_string(index=False))\n\n    # Statistics\n    fallback_count = (submission_df['text'] == \"আমি ভাত খাই।\").sum()\n    print(f\"\\n{'='*60}\")\n    print(f\"Statistics:\")\n    print(f\"  Total files: {len(submission_df)}\")\n    print(f\"  Successful: {len(submission_df) - fallback_count}\")\n    print(f\"  Fallback: {fallback_count}\")\n    print(f\"  Success rate: {100 * (len(submission_df) - fallback_count) / len(submission_df):.1f}%\")\n\n    # Text length statistics\n    submission_df['text_length'] = submission_df['text'].str.len()\n    print(f\"\\nText length statistics:\")\n    print(f\"  Mean: {submission_df['text_length'].mean():.1f} characters\")\n    print(f\"  Min: {submission_df['text_length'].min()}\")\n    print(f\"  Max: {submission_df['text_length'].max()}\")\n    print(f\"{'='*60}\")\n    print(\"✓ Ready for submission!\")","metadata":{"trusted":true,"id":"2Rx8Avo9nTjH","execution":{"iopub.status.busy":"2025-11-16T10:20:50.407981Z","iopub.status.idle":"2025-11-16T10:20:50.408287Z","shell.execute_reply.started":"2025-11-16T10:20:50.408129Z","shell.execute_reply":"2025-11-16T10:20:50.408142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n# from pathlib import Path\n\n# # Path to Kaggle working directory\n# working_dir = Path(\"/kaggle/working\")\n\n# # Confirm current contents before deleting\n# print(\"Files and folders before cleanup:\")\n# for item in working_dir.iterdir():\n#     print(\" -\", item.name)\n\n# # Remove all files and subdirectories inside /kaggle/working\n# for item in working_dir.iterdir():\n#     try:\n#         if item.is_file() or item.is_symlink():\n#             item.unlink()\n#         elif item.is_dir():\n#             shutil.rmtree(item)\n#     except Exception as e:\n#         print(f\"Failed to remove {item}: {e}\")\n\n# print(\"\\n Cleanup complete. Current contents:\")\n# print(list(working_dir.iterdir()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T10:20:50.408917Z","iopub.status.idle":"2025-11-16T10:20:50.409217Z","shell.execute_reply.started":"2025-11-16T10:20:50.409067Z","shell.execute_reply":"2025-11-16T10:20:50.409079Z"}},"outputs":[],"execution_count":null}]}